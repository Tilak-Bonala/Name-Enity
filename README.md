Named Entity Recognition with BiLSTM and Keras 📌 Introduction This project focuses on building a Named Entity Recognition (NER) system using a Bi-directional LSTM (BiLSTM) deep learning architecture. The model is trained on a labeled dataset containing word-level annotations for named entities and aims to classify each token into predefined entity categories such as persons, locations, organizations, etc.

🗂️ Table of Contents Introduction

Data Description

Installation

Usage

Model Architecture

Features

Dependencies

Configuration

Evaluation

Examples

Troubleshooting

Contributors

License

📊 Data Description The dataset, ner_dataset.csv, includes token-level annotations, where each word is labeled with a tag (e.g., O, B-PER, I-LOC, etc.). The dataset is preprocessed by:

Grouping words into sentences

Replacing missing values

Filtering out incomplete entries

The unique words and tags are extracted and encoded for training.

💾 Installation To run this project, use Google Colab or a local Jupyter environment. Ensure all required packages are installed (see Dependencies).

▶️ Usage Upload the dataset file.

Preprocess the data into sequences of words and labels.

Convert words and tags to indices.

Train a BiLSTM model on the data.

Evaluate model accuracy and loss.

Use the trained model to make predictions on new text.

Optionally, compare with spaCy's NER output.

🧠 Model Architecture The model uses a Sequential Keras architecture comprising:

Embedding Layer: Transforms token indices into dense vectors.

Bidirectional LSTM: Captures both past and future context.

LSTM Layer: Adds additional contextual learning.

TimeDistributed Dense Layer: Outputs probabilities across tag categories.

The model is compiled with categorical cross-entropy loss and trained for multiple epochs with a validation split.

🌟 Features End-to-end NER pipeline: from raw data to entity extraction.

Preprocessing handles missing or malformed entries.

BiLSTM-based deep learning approach.

Visualization of predictions using spaCy and displaCy.

Custom prediction function for new input text.

📦 Dependencies Python 3.x

Pandas

NumPy

TensorFlow / Keras

scikit-learn

spaCy (with en_core_web_sm model)

⚙️ Configuration Key configuration parameters include:

Max Sequence Length: Automatically determined from data.

Batch Size: 32

Epochs: 5

Dropout: Applied in LSTM layers

Validation Split: 10%

📈 Evaluation The model is evaluated using:

Validation accuracy

Validation loss

These metrics are printed after training to assess performance on unseen data.

🧪 Examples The trained model can predict tags for user-provided text such as:

"OpenAI is based in San Francisco."

It will return token-tag pairs that indicate named entities. The output can be visualized using spaCy’s displacy.

🛠️ Troubleshooting Model not learning? Ensure the dataset is uploaded correctly and sequences are padded appropriately.

Shape mismatch errors? Double-check sequence length and one-hot encoding steps.

spaCy errors? Confirm the correct language model (en_core_web_sm) is installed.

👥 Contributors Primary Author: [Your Name or GitHub handle]

Notebook Base: Google Colab autogenerated

📜 License This project is distributed for educational and research purposes. Licensing terms depend on the dataset source and third-party libraries used.
