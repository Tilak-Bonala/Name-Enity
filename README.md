Named Entity Recognition with BiLSTM and Keras ğŸ“Œ Introduction This project focuses on building a Named Entity Recognition (NER) system using a Bi-directional LSTM (BiLSTM) deep learning architecture. The model is trained on a labeled dataset containing word-level annotations for named entities and aims to classify each token into predefined entity categories such as persons, locations, organizations, etc.

ğŸ—‚ï¸ Table of Contents Introduction

Data Description

Installation

Usage

Model Architecture

Features

Dependencies

Configuration

Evaluation

Examples

Troubleshooting

Contributors

License

ğŸ“Š Data Description The dataset, ner_dataset.csv, includes token-level annotations, where each word is labeled with a tag (e.g., O, B-PER, I-LOC, etc.). The dataset is preprocessed by:

Grouping words into sentences

Replacing missing values

Filtering out incomplete entries

The unique words and tags are extracted and encoded for training.

ğŸ’¾ Installation To run this project, use Google Colab or a local Jupyter environment. Ensure all required packages are installed (see Dependencies).

â–¶ï¸ Usage Upload the dataset file.

Preprocess the data into sequences of words and labels.

Convert words and tags to indices.

Train a BiLSTM model on the data.

Evaluate model accuracy and loss.

Use the trained model to make predictions on new text.

Optionally, compare with spaCy's NER output.

ğŸ§  Model Architecture The model uses a Sequential Keras architecture comprising:

Embedding Layer: Transforms token indices into dense vectors.

Bidirectional LSTM: Captures both past and future context.

LSTM Layer: Adds additional contextual learning.

TimeDistributed Dense Layer: Outputs probabilities across tag categories.

The model is compiled with categorical cross-entropy loss and trained for multiple epochs with a validation split.

ğŸŒŸ Features End-to-end NER pipeline: from raw data to entity extraction.

Preprocessing handles missing or malformed entries.

BiLSTM-based deep learning approach.

Visualization of predictions using spaCy and displaCy.

Custom prediction function for new input text.

ğŸ“¦ Dependencies Python 3.x

Pandas

NumPy

TensorFlow / Keras

scikit-learn

spaCy (with en_core_web_sm model)

âš™ï¸ Configuration Key configuration parameters include:

Max Sequence Length: Automatically determined from data.

Batch Size: 32

Epochs: 5

Dropout: Applied in LSTM layers

Validation Split: 10%

ğŸ“ˆ Evaluation The model is evaluated using:

Validation accuracy

Validation loss

These metrics are printed after training to assess performance on unseen data.

ğŸ§ª Examples The trained model can predict tags for user-provided text such as:

"OpenAI is based in San Francisco."

It will return token-tag pairs that indicate named entities. The output can be visualized using spaCyâ€™s displacy.

ğŸ› ï¸ Troubleshooting Model not learning? Ensure the dataset is uploaded correctly and sequences are padded appropriately.

Shape mismatch errors? Double-check sequence length and one-hot encoding steps.

spaCy errors? Confirm the correct language model (en_core_web_sm) is installed.

ğŸ‘¥ Contributors Primary Author: [Your Name or GitHub handle]

Notebook Base: Google Colab autogenerated

ğŸ“œ License This project is distributed for educational and research purposes. Licensing terms depend on the dataset source and third-party libraries used.
